# ingest_cli.py

from __future__ import annotations

import argparse
import json
import os
import uuid
import hashlib
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, Dict, List, Tuple

from tqdm import tqdm

# Haystack 2.x
from haystack.dataclasses import Document
from haystack.components.embedders import SentenceTransformersDocumentEmbedder
from haystack.utils.device import ComponentDevice

# Qdrantï¼ˆæ£€ç´¢å­˜åœ¨æ€§ç”¨ clientã€å†™å…¥ç”¨ DocumentStoreï¼‰
try:
    from haystack_integrations.document_stores.qdrant import QdrantDocumentStore
except Exception:
    from qdrant_haystack import QdrantDocumentStore
from qdrant_client import QdrantClient

import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# ========= å¸¸é‡ï¼ˆå¯æŒ‰æœºå™¨è°ƒï¼‰=========
FILE_CHUNK   = 500   # æ¯æ‰¹å¤„ç†çš„æ–‡ä»¶æ•°ï¼›å¯æŒ‰æœºå™¨æ”¹ 500~5000

# ========= å¸¸é‡ï¼ˆå¯æŒ‰ç¡¬ä»¶è°ƒä¼˜ï¼‰=========
EMBED_MODEL   = "Path/to/Qwen3-Embedding-0.6B"   # ä½ çš„æœ¬åœ°æ¨¡å‹ï¼ˆä¿æŒä¸å˜ï¼‰
EMBED_DIM     = 1024
CUDA0         = ComponentDevice.from_str("cuda:0")  # å¦‚éœ€ CPU å¯æ”¹ "cpu"

EMBED_BATCH   = 4     # æ¨¡å‹å†…éƒ¨ batch_sizeï¼ˆæ˜¾å­˜è¶³å¤Ÿå¯æåˆ° 256ï¼‰
OUTER_CHUNK   = 4000    # å¤–å±‚ä¸€æ¬¡ run çš„æ–‡æ¡£æ¡æ•°ï¼ˆå¯æåˆ° 5000ï¼‰
UPSERT_BATCH  = 2000    # Qdrant upsert æ¯æ‰¹æ¡æ•°

# ========= Qdrant Server =========
QDRANT_URL    = os.environ.get("QDRANT_URL", "http://127.0.0.1:6333")
QDRANT_API_KEY= os.environ.get("QDRANT_API_KEY", None)
USE_GRPC      = False   # å›ºå®š HTTPï¼Œé¿å…è¢«ç³»ç»Ÿä»£ç†åŠ«æŒåˆ° 7890

# ========= æ•°æ®æºè·¯å¾„ =========
JSON_FOLDER       = '***'
JSON_SOURCE_NAME  = '***'

JSON_FOLDER_1     = '***'
JSON_SOURCE_NAME_1= '***'

TXT_FOLDER_DEFAULT= '***'
TXT_SOURCE_NAME   = '***'

TXT_FOLDER_SHUOWEN= '***'
TXT_SOURCE_NAME_SHUOWEN = '***'


# ========= å·¥å…·å‡½æ•° =========
def stable_id_uuid5(*parts: str) -> str:
    """ç¡®å®šæ€§ UUIDv5ï¼ˆQdrant å…è®¸çš„ point id ç±»å‹ï¼Œä¸”ç¨³å®šå¯å¤ç°ï¼‰"""
    return str(uuid.uuid5(uuid.NAMESPACE_URL, "|".join(parts)))

def content_sha1(text: str) -> str:
    return hashlib.sha1((text or "").encode("utf-8")).hexdigest()

def qdrant_client() -> QdrantClient:
    # æ¸…ç†ä»£ç†ï¼ˆé¿å…èµ° 7890ï¼‰
    for k in ["HTTP_PROXY","http_proxy","HTTPS_PROXY","https_proxy","ALL_PROXY","all_proxy"]:
        os.environ.pop(k, None)
    os.environ["NO_PROXY"] = "127.0.0.1,localhost"
    return QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, prefer_grpc=USE_GRPC, timeout=60)

def qdrant_store(collection: str) -> QdrantDocumentStore:
    client = qdrant_client()
    try:
        return QdrantDocumentStore(
            client=client,
            index=collection,
            embedding_dim=EMBED_DIM,
            similarity="cosine",
            recreate_index=False,
            return_embedding=False,
            hnsw_config={"m": 32, "ef_construct": 256},
        )
    except TypeError:
        # å…¼å®¹æ—§ç‰ˆ qdrant-haystack
        return QdrantDocumentStore(
            url=QDRANT_URL,
            api_key=QDRANT_API_KEY,
            index=collection,
            embedding_dim=EMBED_DIM,
            similarity="cosine",
            recreate_index=False,
            return_embedding=False,
            hnsw_config={"m": 32, "ef_construct": 256},
            prefer_grpc=USE_GRPC,
            timeout=60,
        )

def chunked(iterable, size):
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf


# ========= Splitters =========
@dataclass
class SourceConfig:
    name: str
    folder: Path
    suffix: str
    splitter: Callable[[List[Path], str], List[Document]]

def splitter_json_blocks(files: List[Path], src_name: str) -> List[Document]:
    """
    åªç´¢å¼•ï¼šblock_label == 'text' ä¸” æ–‡æœ¬é•¿åº¦ > 15 çš„å—
    å¹¶ä¿ç•™ä½ç½®ä¿¡æ¯ç­‰ meta å­—æ®µã€‚
    """
    ALLOWED_LABELS = {"text"}
    MIN_TEXT_LEN = 15

    docs: List[Document] = []
    kept, skipped_short, skipped_label = 0, 0, 0

    for fp in files:
        try:
            pages = json.loads(fp.read_text(encoding="utf-8"))
        except Exception:
            print(f"âš ï¸ è·³è¿‡æŸå JSONï¼š{fp}")
            continue
        if not isinstance(pages, list):
            continue

        for page in pages:
            if not isinstance(page, dict):
                continue
            page_id = page.get("page_id")
            page_res = page.get("page_resolution")
            input_image = page.get("input_image")
            results = page.get("result", [])
            width = height = None
            if isinstance(page_res, dict):
                try:
                    width = int(page_res.get("width") or 0) or None
                    height = int(page_res.get("height") or 0) or None
                except Exception:
                    width = height = None

            for block_id, block in enumerate(results if isinstance(results, list) else []):
                if not isinstance(block, dict):
                    continue

                label = str(block.get("block_label") or "").strip().lower()
                if label not in ALLOWED_LABELS:
                    skipped_label += 1
                    continue

                text = (block.get("block_content") or "").strip()
                if len(text) <= MIN_TEXT_LEN:
                    skipped_short += 1
                    continue

                block_bbox = block.get("block_bbox")
                line_boxes = block.get("line_boxes", [])
                block_bbox_norm = None
                if (
                    isinstance(block_bbox, (list, tuple)) and len(block_bbox) == 4
                    and isinstance(width, int) and width and isinstance(height, int) and height
                ):
                    try:
                        x1, y1, x2, y2 = map(float, block_bbox)
                        block_bbox_norm = [x1/width, y1/height, x2/width, y2/height]
                    except Exception:
                        block_bbox_norm = None

                meta = {
                    "source": src_name,
                    "file_name": fp.name,
                    "page_id": page_id,
                    "page_resolution": page_res,
                    "input_image": input_image,
                    "block_id": block_id,
                    "block_label": label,
                    "block_bbox": block_bbox,
                    "block_bbox_norm": block_bbox_norm,
                    "line_boxes": line_boxes,
                }
                # ç»Ÿä¸€ç”¨ UUIDv5ï¼Œæ»¡è¶³ Qdrant è¦æ±‚
                doc_id = stable_id_uuid5(src_name, fp.name, str(page_id or ""), str(block_id))
                docs.append(Document(id=doc_id, content=text, meta=meta))
                kept += 1

    print(f"ğŸ“Š splitter_json_blocks: kept={kept}, skipped_label={skipped_label}, skipped_short={skipped_short}")
    return docs

def splitter_default(files: List[Path], src_name: str) -> List[Document]:
    docs, seen_modern = [], set()
    for fp in files:
        try:
            raw = fp.read_text(encoding="utf-8")
        except Exception:
            print(f"âš ï¸ è·³è¿‡æŸå TXTï¼š{fp}")
            continue
        for blk in raw.strip().split("\n\n"):
            lines = [ln.strip() for ln in blk.split("\n") if ln.strip()]
            if len(lines) == 2 and lines[0].startswith("å¤æ–‡ï¼š") and lines[1].startswith("ç°ä»£æ–‡ï¼š"):
                klass = lines[0][3:].strip()
                modern = lines[1][4:].strip()
                if modern and modern not in seen_modern:
                    seen_modern.add(modern)
                    meta = {"source": src_name, "classical_text": klass, "file_name": fp.name}
                    # ç”¨å†…å®¹ hash åš ID çš„ç»„æˆéƒ¨åˆ†ï¼Œä½†æœ€ç»ˆä»ç”Ÿæˆ UUIDv5
                    doc_id = stable_id_uuid5(src_name, fp.name, content_sha1(modern))
                    docs.append(Document(id=doc_id, content=modern, meta=meta))
    return docs

def splitter_shuowen(files: List[Path], src_name: str) -> List[Document]:
    """
    æ•´æ–‡ä»¶å†™å…¥ï¼ˆå¦‚â€œè¯´æ–‡è§£å­—å­—å¤´â€ï¼‰ï¼ŒID ç”¨ UUIDv5ã€‚
    """
    docs = []
    for fp in files:
        try:
            text = fp.read_text(encoding="utf-8").strip()
        except Exception:
            print(f"âš ï¸ è·³è¿‡æŸå TXTï¼š{fp}")
            continue
        if text:
            meta = {"source": src_name, "entry": fp.stem, "file_name": fp.name}
            doc_id = stable_id_uuid5(src_name, fp.name)
            docs.append(Document(id=doc_id, content=text, meta=meta))
    return docs

SOURCES: Dict[str, SourceConfig] = {
    JSON_SOURCE_NAME_1:      SourceConfig(JSON_SOURCE_NAME_1,      JSON_FOLDER_1,      ".json", splitter_json_blocks),
    JSON_SOURCE_NAME:        SourceConfig(JSON_SOURCE_NAME,        JSON_FOLDER,        ".json", splitter_json_blocks),
    TXT_SOURCE_NAME:         SourceConfig(TXT_SOURCE_NAME,         TXT_FOLDER_DEFAULT, ".txt",  splitter_default),
    TXT_SOURCE_NAME_SHUOWEN: SourceConfig(TXT_SOURCE_NAME_SHUOWEN, TXT_FOLDER_SHUOWEN, ".txt",  splitter_shuowen),
}


# ========= å¯¹è´¦ï¼ˆä¸ Server æ¯”è¾ƒï¼‰ï¼šæ‰¾å‡º MISSING / CHANGED =========
def diff_against_server(collection: str, docs: List[Document]) -> Tuple[List[Document], List[Document], int]:
    """
    è¿”å›ï¼š(missing_docs, changed_docs, same_count)
    ä¾æ®ï¼šid æ˜¯å¦å­˜åœ¨ï¼›å­˜åœ¨åˆ™æ¯”è¾ƒ payload.content_hash æ˜¯å¦ä¸€è‡´
    """
    client = qdrant_client()
    id_to_doc = {d.id: d for d in docs}

    # é¢„å…ˆç”Ÿæˆ content_hash æ”¾å…¥ metaï¼ˆpayloadï¼‰
    for d in docs:
        if d.meta is None:
            d.meta = {}
        d.meta["content_hash"] = content_sha1(d.content)

    missing, changed = [], []
    same_cnt = 0

    for batch_ids in tqdm(list(chunked(id_to_doc.keys(), 10000)), desc="å¯¹è´¦:retrieve", unit="batch"):
        try:
            points = client.retrieve(collection_name=collection, ids=batch_ids, with_payload=True, with_vectors=False)
        except Exception as e:
            print(f"âš ï¸ retrieve æ‰¹å¤±è´¥ï¼š{e}")
            points = []

        exist_ids = set()
        for p in points:
            pid = str(p.id)
            exist_ids.add(pid)
            old_hash = (p.payload or {}).get("content_hash")
            new_hash = id_to_doc[pid].meta.get("content_hash")
            if old_hash == new_hash:
                same_cnt += 1
            else:
                changed.append(id_to_doc[pid])

        for pid in batch_ids:
            if pid not in exist_ids:
                missing.append(id_to_doc[pid])
    return missing, changed, same_cnt


# ========= ä»…è®¡ç®—éœ€è¦çš„ embeddingï¼ˆä¸ä½¿ç”¨ pklï¼‰=========
def ensure_embeddings(source: str, docs_need: List[Document]) -> List[Document]:
    """
    Server-only æ¨¡å¼ï¼š
    - ä¸å†è¯»å–/å†™å› pkl
    - ä»…å¯¹ missing/changed æ–‡æ¡£è®¡ç®— embedding åè¿”å›
    """
    if not docs_need:
        return []

    # å¯é€‰ï¼šå¼€å¯ TF32ï¼ˆAmpere+ æ˜æ˜¾æé€Ÿï¼‰
    try:
        import torch
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    except Exception:
        pass

    embedder = SentenceTransformersDocumentEmbedder(
        model=EMBED_MODEL,
        batch_size=EMBED_BATCH,
        device=CUDA0
    )
    embedder.warm_up()
    print(f"ğŸ§® éœ€è¦æ–°è®¡ç®—å‘é‡ï¼š{len(docs_need)}")

    embedded_docs: List[Document] = []
    for batch in tqdm(list(chunked(docs_need, OUTER_CHUNK)), desc="embed", unit="batch"):
        res = embedder.run(documents=batch)
        embedded_docs.extend(res.get("documents", batch))
    return embedded_docs


# ========= upsert å†™å…¥ =========
def upsert_documents(collection: str, docs: List[Document], batch_size: int = UPSERT_BATCH) -> int:
    store = qdrant_store(collection)
    written = 0
    for batch in tqdm(list(chunked(docs, batch_size)), desc="upsert", unit="batch"):
        try:
            store.write_documents(batch, policy="upsert")
            written += len(batch)
        except Exception as e:
            print(f"âš ï¸ upsert æ‰¹å¤±è´¥ï¼š{e}")
    return written


# ========= ä¸»æµç¨‹ =========
def ingest_one_source(source: str) -> None:
    scfg = SOURCES.get(source)
    if not scfg:
        print(f"âŒ æœªçŸ¥ source: {source}ï¼Œå¯é€‰ï¼š{list(SOURCES.keys())}")
        return
    if not scfg.folder.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨ï¼š{scfg.folder}")
        return

    # ä¸€æ¬¡åªæ‹¿ FILE_CHUNK ä¸ªæ–‡ä»¶ï¼Œæµå¼å¤„ç†
    all_files = sorted(scfg.folder.glob(f"*{scfg.suffix}"))
    total_files = len(all_files)
    if total_files == 0:
        print(f"â„¹ï¸ æ— æ–‡ä»¶ï¼š{scfg.folder} (*{scfg.suffix})")
        return

    print(f"ğŸ“ æ‰«æ {source}ï¼š{total_files} ä¸ªæ–‡ä»¶ï¼ˆæŒ‰ {FILE_CHUNK} ä¸€æ‰¹å¤„ç†ï¼‰")

    processed_files = 0
    total_scanned = total_same = total_changed = total_missing = 0
    total_written = 0

    # é€æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å†…å­˜/æ—¶é—´è¿‡å¤§
    for chunk_idx, file_chunk in enumerate(chunked(all_files, FILE_CHUNK), start=1):
        # 1) è§£ææœ¬æ‰¹æ–‡ä»¶ â†’ docs
        docs = scfg.splitter(file_chunk, source)
        if not docs:
            print(f"â„¹ï¸ ç¬¬ {chunk_idx} æ‰¹æœªäº§ç”Ÿæ–‡æœ¬å—")
            processed_files += len(file_chunk)
            continue

        # 2) å¯¹è´¦ï¼šåªæ‰¾æœ¬æ‰¹çš„ missing/changed
        missing, changed, same_cnt = diff_against_server(source, docs)
        scanned = len(docs)
        need_write = changed + missing

        total_scanned += scanned
        total_same    += same_cnt
        total_changed += len(changed)
        total_missing += len(missing)

        print(
            f"ğŸ§¾ ç¬¬ {chunk_idx} æ‰¹ï¼šfiles={len(file_chunk)}  scanned={scanned}  "
            f"same={same_cnt}  changed={len(changed)}  missing={len(missing)}"
        )

        if need_write:
            # 3) ä»…å¯¹æœ¬æ‰¹éœ€è¦å†™å…¥çš„åš embedding
            docs_ready = ensure_embeddings(source, need_write)
            # 4) upsert åˆ° Qdrant
            written = upsert_documents(source, docs_ready, batch_size=UPSERT_BATCH)
            total_written += written
            print(f"âœ… ç¬¬ {chunk_idx} æ‰¹å†™å…¥ï¼š{written} / {len(docs_ready)}")
        else:
            print(f"âœ… ç¬¬ {chunk_idx} æ‰¹ï¼šå…¨éƒ¨ up-to-date")

        processed_files += len(file_chunk)

        # 5) é‡Šæ”¾æ˜¾å­˜/å†…å­˜ï¼ˆå¯é€‰ï¼Œé•¿è·‘æ›´ç¨³ï¼‰
        try:
            import torch, gc
            torch.cuda.empty_cache()
            gc.collect()
        except Exception:
            pass

        # æ‰¹é—´è¿›åº¦
        print(
            f"ğŸ“Š è¿›åº¦ï¼š{processed_files}/{total_files} files  |  "
            f"scanned={total_scanned}  same={total_same}  "
            f"changed={total_changed}  missing={total_missing}  "
            f"written={total_written}"
        )

    print(
        f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼šfiles={total_files}  scanned={total_scanned}  same={total_same}  "
        f"changed={total_changed}  missing={total_missing}  written={total_written}"
    )



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--source", required=True, help=f"å¯é€‰ï¼š{list(SOURCES.keys())}")
    args = parser.parse_args()
    ingest_one_source(args.source)
